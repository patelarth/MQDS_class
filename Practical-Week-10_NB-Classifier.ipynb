{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayes methods are a set of supervised learning algorithms based on applying Bayes' theorem with the \"naive\" assumption of conditional independence between every pair of features given the value of the class variable. Bayes'theorem states the following relationship, given class variable $y$ and dependent feature vector $x_1$ through $x_n$,:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$P(y \\mid x_1, \\dots, x_n) = \\frac{P(y) P(x_1, \\dots x_n \\mid y)}\n",
    "                                 {P(x_1, \\dots, x_n)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the naive conditional independence assumption, we have"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\begin{align}\\begin{aligned}P(y \\mid x_1, \\dots, x_n) \\propto P(y) \\prod_{i=1}^{n} P(x_i \\mid y)\\\\\\Downarrow\\\\\\hat{y} = \\arg\\max_y P(y) \\prod_{i=1}^{n} P(x_i \\mid y),\\end{aligned}\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we can use Maximum A Posteriori (MAP) estimation to estimate $P(y)$ and $P(x_i \\mid y)$; the former is then the relative frequency of class $y$ in the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*References*:\n",
    "H. Zhang (2004). The optimality of Naive Bayes. Proc. FLAIRS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Gaussian Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[GaussianNB](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html#sklearn.naive_bayes.GaussianNB) implements the Gaussian Naive Bayes algorithm for classification on the data sets where features are continuous.   \n",
    "The likelihood of the features is assumed to be Gaussian:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$P(x_i \\mid y) = \\frac{1}{\\sqrt{2\\pi\\sigma^2_y}} \\exp\\left(-\\frac{(x_i - \\mu_y)^2}{2\\sigma^2_y}\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parameters $\\sigma_y$ and $\\mu_y$  are estimated using maximum likelihood."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Demo\n",
    "In this demo, we show how to build a Gaussian Naive Bayes classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Generate a synthetica 2D dataset\n",
    "X, y = make_classification(n_samples=50, n_features=2, n_informative=2,\n",
    "                           n_redundant=0, n_classes=3, n_clusters_per_class=1, \n",
    "                           weights=None, flip_y=0.01, class_sep=0.5, hypercube=True,\n",
    "                           shift=0.0, scale=1.0, shuffle=True, random_state=42)\n",
    "\n",
    "# Data split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, random_state=42)\n",
    "\n",
    "# Visualize the generated data\n",
    "colors = ['blue', 'yellow', 'green']\n",
    "for i, color in enumerate(colors):\n",
    "    plt.scatter(X_train[y_train == i, 0], X_train[y_train == i, 1], c=color)\n",
    "plt.scatter(X_test[:, 0], X_test[:,1], c='red', marker='x', label='Testing Data')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing accuracy is:  0.8\n"
     ]
    }
   ],
   "source": [
    "# Create and training a Gaussian Naive Bayes classifier model\n",
    "clf = GaussianNB()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Use the model to predict testing data\n",
    "y_pred = clf.predict(X_test)\n",
    "accuracy = accuracy_score(y_pred, y_test)\n",
    "print('Testing accuracy is: ', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated probability of classess: \n",
      " [0.325 0.375 0.3  ]\n",
      "Estimated mean for each Gaussian distribution: \n",
      " [[ 0.60903899 -0.56115715]\n",
      " [ 0.39670288  0.51301944]\n",
      " [-0.40161257 -0.83685934]]\n",
      "Estimated variance for each Gaussian distribution: \n",
      " [[0.23233913 1.0483911 ]\n",
      " [0.93521181 0.0662763 ]\n",
      " [0.3309853  0.6755908 ]]\n"
     ]
    }
   ],
   "source": [
    "# Explore the learned probability (model parameters)\n",
    "print('Estimated probability of classess: \\n', clf.class_prior_)\n",
    "print('Estimated mean for each Gaussian distribution: \\n', clf.theta_)\n",
    "print('Estimated variance for each Gaussian distribution: \\n', clf.sigma_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, for Class 0 and the first feature, we can have the following Gaussian disribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$P(x_0 \\mid Class=0) = \\frac{1}{\\sqrt{2\\pi\\cdot0.2323}} \\exp\\left(-\\frac{(x_0 - 0.6090)^2}{2\\cdot0.2323}\\right)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gaussian Naive Bayes accuracy: 0.6967 +- 0.1773\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Use 10-fold cross validation to show a more robust prediction accuracy\n",
    "from sklearn.model_selection import cross_val_score\n",
    "clf = GaussianNB()\n",
    "scores = cross_val_score(clf, X, y, scoring='accuracy', cv=10)\n",
    "print('Gaussian Naive Bayes accuracy: %.4f +- %.4f\\n' % (scores.mean(), scores.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tasks** - The training data is generated as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Means estaimated manually: \n",
      " [[-2.         -1.33333333]\n",
      " [ 2.          1.33333333]]\n",
      "Variances estaimated manually: \n",
      " [[0.66666667 0.22222222]\n",
      " [0.66666667 0.22222222]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\n",
    "y = np.array([1, 1, 1, 2, 2, 2])\n",
    "\n",
    "# Firstly, let's do the parameter estimation manually without using the model\n",
    "X_0_C_1=X[y==1][:,0]\n",
    "X_1_C_1=X[y==1][:,1]\n",
    "X_0_C_2=X[y==2][:,0]\n",
    "X_1_C_2=X[y==2][:,1]\n",
    "\n",
    "manual_means = np.array([[X_0_C_1.mean(), X_1_C_1.mean()], [X_0_C_2.mean(), X_1_C_2.mean()]])\n",
    "print('Means estaimated manually: \\n', manual_means)\n",
    "manual_vars = np.array([[X_0_C_1.var(), X_1_C_1.var()], [X_0_C_2.var(), X_1_C_2.var()]])\n",
    "print('Variances estaimated manually: \\n', manual_vars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1**: Training a GaussianNB model and print out the learned model parameters (parameters of probability distributions). And check if the learned parameters comply with the manually estimated ones as shown above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianNB(priors=None, var_smoothing=1e-09)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create and training a Gaussian Naive Bayes classifier model\n",
    "task1clf = GaussianNB()\n",
    "task1clf.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Means value :\n",
      " [ 0.0 1.5 \n",
      " 0.0 1.5 ]\n",
      "Variances value :\n",
      " [ 1.3228756555322954 0.5 \n",
      " 2.5495097567963922 0.5 ]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_0,x_1,y_0,y_1 = train_test_split(X,y)\n",
    "print(\"Means value :\\n\",'[',x_0.mean(),y_0.mean(),\"\\n\",x_1.mean(),y_0.mean(),']')\n",
    "print(\"Variances value :\\n\",'[',x_0.std(),y_0.std(),\"\\n\",x_1.std(),y_0.std(),']')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q2**: Predict the label of a data [-0.8,-1]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n"
     ]
    }
   ],
   "source": [
    "#predicting the value of the label of data [[-0.8,-1]]\n",
    "print(task1clf.predict([[-0.8, -1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 MultinomialNB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [MultinomialNB](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html#sklearn.naive_bayes.MultinomialNB) classification model is suitable for classification with discrete features. To let the model handle to categorical data, we often need to transform the categorical values to numberic ones, through [encoding](https://scikit-learn.org/stable/modules/preprocessing.html#encoding-categorical-features) methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Outlook Humidity    Wind Play\n",
      "0     Sunny     High    Weak   No\n",
      "1     Sunny     High  Strong   No\n",
      "2  Overcast     High    Weak  Yes\n",
      "3      Rain     High    Weak  Yes\n",
      "4      Rain   Normal    Weak  Yes\n",
      "\n",
      "Data shape:  (14, 4)\n"
     ]
    }
   ],
   "source": [
    "# Load the weather data\n",
    "weather_data = pd.read_csv('files/weather.csv')\n",
    "print(weather_data.head())\n",
    "print('\\nData shape: ', weather_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data pre-processing and preparation\n",
    "# Firstly, we need to convert the date from being categorical to being numerical\n",
    "import sklearn.preprocessing \n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "enc = OrdinalEncoder()\n",
    "coded_data = enc.fit_transform(weather_data)\n",
    "\n",
    "X = coded_data[:, 0:-1]\n",
    "y = coded_data[:, -1]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creat and train a model\n",
    "clf_mnb = MultinomialNB()\n",
    "clf_mnb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is: 0.7500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "y_pred = clf_mnb.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print('Accuracy is: %.4f\\n' % accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated probability of classess: \n",
      " [0.4 0.6]\n",
      "Estimated class-conditional probabilities for each feature: \n",
      " [[0.63636364 0.18181818 0.18181818]\n",
      " [0.41176471 0.29411765 0.29411765]]\n"
     ]
    }
   ],
   "source": [
    "# Explore the learned model parameters (probabilities)\n",
    "# Note that the probabilities are in the logorithmic form. Why? The log-sum-exp trick for underflow of probability products\n",
    "print('Estimated probability of classess: \\n', np.e**clf_mnb.class_log_prior_)\n",
    "print('Estimated class-conditional probabilities for each feature: \\n', np.e**clf_mnb.feature_log_prob_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tasks** - The training data is generated as follows. The number of data instances (6) is small while the demensionality of the data is relatively highly (100)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a synthetic data set\n",
    "import numpy as np\n",
    "X = np.random.randint(5, size=(6, 100))\n",
    "y = np.array([1, 2, 3, 4, 5, 6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q3**: Training a MultinomialNB model, and predict the label of a data X_new = [[1,2,1,0,2,3,0,3,2,1,1,3,3,0,4,2,2,0,0,2,2,3,4,4,4,4,0,3,3,\n",
    "          1,1,1,2,3,1,3,0,2,2,0,4,2,4,3,2,0,1,1,1,2,3,0,0,3,4,3,3,4,\n",
    "          2,1,0,0,0,0,4,1,2,0,0,4,4,0,4,1,3,1,1,1,3,1,1,1,4,3,1,1,3,\n",
    "          2,0,0,0,3,4,1,1,4,3,2,3,4]]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted value :  [1]\n"
     ]
    }
   ],
   "source": [
    "# Training a MultinomialNB model\n",
    "q3clf = MultinomialNB()\n",
    "q3clf.fit(X,y)\n",
    "\n",
    "# Predict the class of the new data instance\n",
    "X_new = [[1,2,1,0,2,3,0,3,2,1,1,3,3,0,4,2,2,0,0,2,2,3,4,4,4,4,0,3,3,\n",
    "          1,1,1,2,3,1,3,0,2,2,0,4,2,4,3,2,0,1,1,1,2,3,0,0,3,4,3,3,4,\n",
    "          2,1,0,0,0,0,4,1,2,0,0,4,4,0,4,1,3,1,1,1,3,1,1,1,4,3,1,1,3,\n",
    "          2,0,0,0,3,4,1,1,4,3,2,3,4]]\n",
    "X_new_predicted = q3clf.predict(X_new)\n",
    "print(\"predicted value : \",X_new_predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q4**: In our lecture, we discussed that if there is no occurence of some feature values, zero probabilities will appear. To overcome this issue, Laplace correction (smoothing) is proposed, as shown in the follow formula. In the [MultinomialNB](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html#sklearn.naive_bayes.MultinomialNB) implementation, the parameter 'alpha' controls the way we apply Laplace smoothing. The default value is 'alpha=1.0'. Please create and train a model with no Laplace smoothing for the above data set. Check if there are zero probabilities (note that due to the accuracy issue, zero might be represented as a signficantly small number by the computer), and compare the leaned model parameters (probabilities) with the case 'alpha=1' \n",
    "$$p(x_{yi}|y)=\\frac{N_{yi}+\\alpha}{N_y+{\\alpha}n}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create and train a MultinomialNB model with no Laplace smoothing\n",
    "q4clf = MultinomialNB(alpha=0)\n",
    "q4clf.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-672.79588099, -784.43941748, -372.95947443, -552.20140618,\n",
       "        -790.64956738,    0.        ]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_q4 = q4clf.predict_log_proba(X_new)\n",
    "X_q4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Process on 'Iris' Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Week 9, we have studied how to use KNN algorithm to do classification task on 'iris' data. Here,we are going to employ the GaussianNB to conduct the same task. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "iris_data = load_iris()\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris_data['data'], iris_data['target'], random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q4**ï¼šReport the prediction result on test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train a model and do the prediction\n",
    "irisclf = GaussianNB()\n",
    "irisclf.fit(X_train, y_train)\n",
    "pred_y = irisclf.predict(X_test)\n",
    "accuracy_score(y_test,pred_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the features value [5.8, 2., 4. , 1.2] apply on the trained model and got a prediction value :  [1]  class\n"
     ]
    }
   ],
   "source": [
    "print(\"the features value [5.8, 2., 4. , 1.2] apply on the trained model and got a prediction value : \",irisclf.predict([[5.8, 2., 4. , 1.2]]),\" class\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q5**: Compare the prediction accuaracy between KNN clasifier (use the optimal K you've identied) and Gaussian Naive Bayes. Use 10-cross validation to report the accuracy mean and standard deviation (Note this is to ensure the comparison is based on robust performace). Which classifidation mdoel is more accurate on Iris data set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the two types of model\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "means = []\n",
    "cv_scores = []\n",
    "for i in range(1,11):\n",
    "    knn = KNeighborsClassifier(n_neighbors=i)\n",
    "    scores = cross_val_score(knn,X_train,y_train, cv = 10, scoring ='accuracy')\n",
    "    cv_scores.append(scores)\n",
    "    means.append(scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q6**: Can we use Multinomial Naive Bayes classifiation model for Iris data? Yes! We can discretize continuous features by use the [KBinsDiscretizer](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.KBinsDiscretizer.html#sklearn.preprocessing.KBinsDiscretizer) method. Note that you can different ways of discrization. Please report the prediction result with the following discritization method. Also, try another discritization method with parameter 'encode=onehot', and report the prediction result. Use 10-cross validation to report the accuracy mean and standard deviation as we did above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score of using KBinsDiscretizer encode = ordinal 0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "# Discretize the continous attributes apply encode as ordinal\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "encoder = KBinsDiscretizer(n_bins=5, encode='ordinal', strategy='uniform')\n",
    "X = encoder.fit_transform(iris_data['data'])\n",
    "y = iris_data['target']\n",
    "\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.20)\n",
    "\n",
    "multiclf = MultinomialNB()\n",
    "multiclf.fit(X_train,y_train)\n",
    "y_pred = multiclf.predict(X_test)\n",
    "print('Accuracy score of using KBinsDiscretizer encode = ordinal',accuracy_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score of using KBinsDiscretizer encode = onehot :  0.8666666666666667\n"
     ]
    }
   ],
   "source": [
    "# Discretize the continous attributes apply encode as onehot\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "encoder = KBinsDiscretizer(n_bins=5, encode='onehot', strategy='uniform')\n",
    "X = encoder.fit_transform(iris_data['data'])\n",
    "y = iris_data['target']\n",
    "\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.20)\n",
    "\n",
    "multiclf = MultinomialNB()\n",
    "multiclf.fit(X_train,y_train)\n",
    "y_pred = multiclf.predict(X_test)\n",
    "print(\"Accuracy score of using KBinsDiscretizer encode = onehot : \",accuracy_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Predict Human Activity Recognition (HAR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objective of this practice exercise is to predict current human activity based on phisiological activity measurements from 53 different features based in the [HAR dataset](http://groupware.les.inf.puc-rio.br/har#sbia_paper_section). The training (`har_train.csv`) and test (`har_validate.csv`) datasets are provided."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q7**: Build a Naive Bayes model, predict on the test dataset and compute the [confusion matrix](https://towardsdatascience.com/understanding-confusion-matrix-a9ad42dcfd62). Note: Please refer to the [`sklearn.metrics.confusion_matrix`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html). This is a check point question for this week's workshop. You need to report accuracy_scores on train and test set of Human Activity Recognition dataset. Also provide confusion matrix on test set and provide a brief interpretation of your results based on accuracy scores and confusion matrix (which class is misclassified into what). Your description should not be more than a paragraph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "har_train = pd.read_csv(\"files/har_train.csv\")\n",
    "har_test = pd.read_csv(\"files/har_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['classe', 'roll_belt', 'pitch_belt', 'yaw_belt', 'total_accel_belt',\n",
      "       'gyros_belt_x', 'gyros_belt_y', 'gyros_belt_z', 'accel_belt_x',\n",
      "       'accel_belt_y', 'accel_belt_z', 'magnet_belt_x', 'magnet_belt_y',\n",
      "       'magnet_belt_z', 'roll_arm', 'pitch_arm', 'yaw_arm', 'total_accel_arm',\n",
      "       'gyros_arm_x', 'gyros_arm_y', 'gyros_arm_z', 'accel_arm_x',\n",
      "       'accel_arm_y', 'accel_arm_z', 'magnet_arm_x', 'magnet_arm_y',\n",
      "       'magnet_arm_z', 'roll_dumbbell', 'pitch_dumbbell', 'yaw_dumbbell',\n",
      "       'total_accel_dumbbell', 'gyros_dumbbell_x', 'gyros_dumbbell_y',\n",
      "       'gyros_dumbbell_z', 'accel_dumbbell_x', 'accel_dumbbell_y',\n",
      "       'accel_dumbbell_z', 'magnet_dumbbell_x', 'magnet_dumbbell_y',\n",
      "       'magnet_dumbbell_z', 'roll_forearm', 'pitch_forearm', 'yaw_forearm',\n",
      "       'total_accel_forearm', 'gyros_forearm_x', 'gyros_forearm_y',\n",
      "       'gyros_forearm_z', 'accel_forearm_x', 'accel_forearm_y',\n",
      "       'accel_forearm_z', 'magnet_forearm_x', 'magnet_forearm_y',\n",
      "       'magnet_forearm_z'],\n",
      "      dtype='object')\n",
      "\n",
      " {'A', 'B', 'C', 'D', 'E'}\n"
     ]
    }
   ],
   "source": [
    "# checking all 53 column names in the dataset\n",
    "print(har_train.columns)\n",
    "\n",
    "# The variable to predict (or label) is \"classe\" column. \n",
    "# checking number of classes in classe column\n",
    "print(\"\\n\",set(har_train['classe']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X data shape :  (13737, 52) \n",
      "Y data shape :  (13737,)\n"
     ]
    }
   ],
   "source": [
    "# Separate features and class labels\n",
    "#splitting har_train in x and y \n",
    "X_har,y_har = har_train.drop(columns=['classe']),har_train.classe\n",
    "print(\"X data shape : \",X_har.shape,\"\\nY data shape : \",y_har.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train the model\n",
    "q7clf = GaussianNB()\n",
    "q7clf.fit(X_har,y_har)\n",
    "y_pred = q7clf.predict(har_test.drop(columns=['classe']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score of the Test data :  0.5542905692438402\n",
      "Accuracy score of the Train data :  0.5580548882579893\n"
     ]
    }
   ],
   "source": [
    "# Reporting accuracy score and confusion matrix on test set\n",
    "print(\"Accuracy score of the Test data : \",accuracy_score(har_test.classe,y_pred))\n",
    "print(\"Accuracy score of the Train data : \",accuracy_score(har_train.classe,q7clf.predict(har_train.drop(columns=['classe']))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix of test data :\n",
      " [[1070   95  262  212   35]\n",
      " [ 127  685  145   76  106]\n",
      " [ 223  106  512  136   49]\n",
      " [ 102   35  271  441  115]\n",
      " [  51  239   95  143  554]]\n"
     ]
    }
   ],
   "source": [
    "print(\"confusion matrix of test data :\\n\",confusion_matrix(har_test.classe,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1070   95  262  212   35]   Total =  1674 of class :  A\n",
      "[127 685 145  76 106]   Total =  1139 of class :  B\n",
      "[223 106 512 136  49]   Total =  1082 of class :  E\n",
      "[102  35 271 441 115]   Total =  1026 of class :  C\n",
      "[ 51 239  95 143 554]   Total =  964 of class :  D\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(har_test.classe.value_counts())):\n",
    "    print(confusion_matrix(har_test.classe,y_pred)[i],'  Total = ',har_test.classe.value_counts()[i],'of class : ',har_test.classe.value_counts().index[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true positive rate  0.6391875746714456 of class :  A\n",
      "true positive rate  0.6014047410008779 of class :  B\n",
      "true positive rate  0.4731977818853974 of class :  E\n",
      "true positive rate  0.4298245614035088 of class :  C\n",
      "true positive rate  0.5746887966804979 of class :  D\n"
     ]
    }
   ],
   "source": [
    "#printing true positive rate\n",
    "for i in range(len(har_test.classe.value_counts())):\n",
    "    print(\"true positive rate \",max(confusion_matrix(har_test.classe,y_pred)[i])/har_test.classe.value_counts()[i],'of class : ',har_test.classe.value_counts().index[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "false predicted rate of  :  A  =  0.36081242532855434\n",
      "false predicted rate of  :  B  =  0.398595258999122\n",
      "false predicted rate of  :  E  =  0.5268022181146026\n",
      "false predicted rate of  :  C  =  0.5701754385964912\n",
      "false predicted rate of  :  D  =  0.42531120331950206\n"
     ]
    }
   ],
   "source": [
    "#printing false predicted rate\n",
    "for i in range(len(har_test.classe.value_counts())):\n",
    "    print(\"false predicted rate of  : \",har_test.classe.value_counts().index[i],\" = \",(har_test.classe.value_counts()[i]-max(confusion_matrix(har_test.classe,y_pred)[i]))/har_test.classe.value_counts()[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the model show the Accuracy score of Train data: 0.5580 and Accuracy score of the Test data :  0.5542 which is nearly equal. In confusion matrix there are 5 classes A,B,E,C,D and its show that the true positive value of the class A is 0.63 which is higher among all and followed by B,D,E with value 0.60,0.57,0.47 respectively. the class C show lowest rate with 0.42, so from this we can say that C class make more false prediction than others."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
